{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13251030,"sourceType":"datasetVersion","datasetId":8396580}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 align=\"center\"><font color='green'> üö¥‚Äç‚ôÇÔ∏è Exercise 1: Bike Rental Regression Task</font></h1>\n<h3 align=\"left\"> <font color='purple'>This project focuses on a regression task to predict the total daily count of rental bikes ('cnt' column) based on environmental and seasonal factors.\nThe objective is to preprocess the data, implement and evaluate regression algorithms: Decision Tree and Ridge Regression using MAE, MSE, R¬≤ score standard metrics.\n</font></h3>\n","metadata":{}},{"cell_type":"code","source":"#importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge \nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n#plots configuration \nsns.set_style(\"whitegrid\")\nplt.rcParams['figure.figsize'] = (10, 6)\nplt.rcParams['axes.titlesize'] = 16\nplt.rcParams['axes.labelsize'] = 12\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T21:26:58.629064Z","iopub.execute_input":"2025-10-18T21:26:58.630613Z","iopub.status.idle":"2025-10-18T21:26:58.645222Z","shell.execute_reply.started":"2025-10-18T21:26:58.63054Z","shell.execute_reply":"2025-10-18T21:26:58.644215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-warning\">  \n<b>Data Loading and Preprocessing:</b> We will be loading the dataset, cleaning irrelevant columns, and encode categorical features. Split and scale the data will also be included.\n</div>\n","metadata":{}},{"cell_type":"code","source":"#Load the dataset\ndf = pd.read_csv(\"/kaggle/input/bike-sharing-dataset/day.csv\")\n\n#Initial information first\nprint(\"Original Dataset Shape:\", df.shape)\nprint(\"\\nFeatures and Data Types:\")\nprint(df.info())\n\n#--- PREPROCESSING ---\n#The target variable 'cnt' column is the sum of 'casual' and 'registered'.\n\ncolumn_drop = ['instant', 'dteday', 'casual', 'registered']\ndf_processed = df.drop(column_drop, axis=1)\n\nprint(\"\\n-- Preprocessing Steps --\")\nprint(f\"Columns were dropped to remove irrelevancy: {column_drop}\")\n\n# --- Encoding Categorical Variables ---\n#categ_column are categorical and need one-hot encoding.\ncateg_column = ['season', 'mnth', 'weekday', 'weathersit']\ndf_encoded = pd.get_dummies(df_processed, columns=categ_column, drop_first=True)\n\nprint(\"Encoded categorical features using One-Hot Encoding=drop_first=True.\")\nprint(\"Processed Dataset Shape:\", df_encoded.shape)\n\n#define Features (X) and Target (y)\nX = df_encoded.drop('cnt', axis=1)\ny = df_encoded['cnt']\n\n#now split the data into training and testing sets (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nprint(f\"Train set size: {X_train.shape[0]} records\")\nprint(f\"Test set size: {X_test.shape[0]} records\")\n\n# --- Scaling Numerical Features ---\nnumeric_column = ['temp', 'atemp', 'hum', 'windspeed']\nscaler = StandardScaler()\n\nX_train[numeric_column] = scaler.fit_transform(X_train[numeric_column])\nX_test[numeric_column] = scaler.transform(X_test[numeric_column])\n\nprint(\"scaled continuous features using StandardScaler.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T21:26:58.646848Z","iopub.execute_input":"2025-10-18T21:26:58.647325Z","iopub.status.idle":"2025-10-18T21:26:58.727744Z","shell.execute_reply.started":"2025-10-18T21:26:58.647301Z","shell.execute_reply":"2025-10-18T21:26:58.726707Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background-color: #ffd6eb;\n    border-left: 6px solid #ff69b4;\n    padding: 15px;\n    border-radius: 8px;\n    color: #880e4f;\n    font-family: 'Helvetica Neue', sans-serif;\n    box-shadow: 0 2px 6px rgba(255, 105, 180, 0.3);\n\">\n<div class=\"alert alert-block alert-info\">\n<h1 align=\"center\"> <font color='gray'>Model Training and Evaluation</font></h1>\n<h3 align=\"center\"> <font color='blue'> Train Decision Tree and Ridge Regression and evaluate their performance on both training and testing sets using MAE, MSE, and R¬≤.\n</font></h3>\n</div>\n","metadata":{}},{"cell_type":"code","source":"#1. Decision Tree (Non-linear prediction) and Ridge Regression (Robust linear baseline)\nmodels = {\n    \"Ridge Regression (alpha=1)\": Ridge(alpha=1.0, random_state=42),\n    \"Decision Tree\": DecisionTreeRegressor(random_state=42, max_depth=10) #limiting depth (max_depth=10) prevents overfitting\n}\n\nresults = []\n\ndef evaluate_model(name, model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n\n    #Predictions\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    #Calculate\n    metrics = {\n        'Model': name,\n        'MAE_Train': mean_absolute_error(y_train, y_train_pred),\n        'MAE_Test': mean_absolute_error(y_test, y_test_pred),\n        'MSE_Train': mean_squared_error(y_train, y_train_pred),\n        'MSE_Test': mean_squared_error(y_test, y_test_pred),\n        'R2_Train': r2_score(y_train, y_train_pred),\n        'R2_Test': r2_score(y_test, y_test_pred)\n    }\n    return metrics\n\nfor name, model in models.items():\n    print(f\"Training {name}...\")\n    metrics = evaluate_model(name, model, X_train, y_train, X_test, y_test)\n    results.append(metrics)\n\nresults_datafr = pd.DataFrame(results)\n\n#Comparison table, this is sorted by Test R2 Score\nprint(\"\\n--- Model Performance Comparison ---\")\nresults_datafr_styled = results_datafr.sort_values(by='R2_Test', ascending=False).style.background_gradient(cmap='Blues', subset=['R2_Test'])\nprint(results_datafr_styled)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T21:26:58.729196Z","iopub.execute_input":"2025-10-18T21:26:58.729542Z","iopub.status.idle":"2025-10-18T21:26:58.769709Z","shell.execute_reply.started":"2025-10-18T21:26:58.729511Z","shell.execute_reply":"2025-10-18T21:26:58.768918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1 align=\"center\"> <font color='gray'>Visualizations and interpretation</font></h1>\n<h3 align=\"center\"> <font color='blue'> Visualize the prediction results for the Decision Tree and analyze the error distribution.\n</font></h3>\n</div>","metadata":{}},{"cell_type":"code","source":"model_pre = \"Decision Tree\"\nmodel_pre = models[model_pre]\n\n#Re-train the Decision Tree for final prediction variables\nmodel_pre.fit(X_train, y_train)\ny_test_pred = model_pre.predict(X_test)\n\n# --- Predicted vs. Actual Values Scatter Plot ---\nplt.figure(figsize=(10, 6))\nplt.scatter(y_test, y_test_pred, alpha=0.7, color='teal')\nplt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', lw=2, label='Perfect Prediction Line (y=x)')\nplt.xlabel('Actual Daily Bike Rentals (cnt)')\nplt.ylabel('Predicted Daily Bike Rentals (cnt)')\nplt.title(f'{model_pre}: Predicted vs. Actual Values ')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n# --- Residual (Error) Distribution Plot ---\n#Residuals = Actual Values - Predicted Values\nresiduals = y_test - y_test_pred\n\nplt.figure(figsize=(10, 6))\nsns.histplot(residuals, kde=True, bins=30, color='darkorange')\nplt.axvline(x=0, color='red', linestyle='--', label='Zero Error')\nplt.xlabel('Residuals (Actual - Predicted)')\nplt.ylabel('Frequency')\nplt.title(f'{model_pre}: Distribution of Prediction Errors = Residuals')\nplt.legend()\nplt.show()\n\n# --- Feature Importance ---\nif model_pre == \"Decision Tree\":\n    feat_importances = pd.Series(model_pre.feat_importances_, index=X.columns)\n    feat_importances_sorted = feat_importances.sort_values(ascending=False)\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=feat_importances_sorted.head(10), y=feat_importances_sorted.head(10).index, palette='viridis')\n    plt.title('Top 10 Feature Importances')\n    plt.xlabel('Importance Score')\n    plt.ylabel('Feature')\n    plt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-18T22:03:57.900426Z","iopub.execute_input":"2025-10-18T22:03:57.900719Z","iopub.status.idle":"2025-10-18T22:03:58.794651Z","shell.execute_reply.started":"2025-10-18T22:03:57.900701Z","shell.execute_reply":"2025-10-18T22:03:58.792908Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Interpretation of Visualizations\n\n#### **Predicted vs. Actual Values Scatter Plot**\nThis plot compares the model's predictions on the test set (Y-axis) against the true daily bike counts (X-axis). \n* **Ideal Outcome:** The dots should cluster tightly around the red straight line, which shows a perfect prediction that Predicted = Actual.\n* **Finding:** My findings on the visualizations are that the tight cluster around the line shows that the model has a good prediction and is generalizing well to the data. The slight scatter at the higher end indicates more difficulty predicting peak demand days precisely.\n\n#### **Distribution of Prediction Errors (Residuals)**\nThis histogram shows the frequency of the model's errors (Residuals = Actual - Predicted). \n* **Ideal Outcome:** The distribution should look like a bell curve centered exactly at zero.\n* **Finding:** The distributions showed a centered near 0 in the middle which suggests that the model is unbiased overall, but the distribution shows a slight skew, suggesting that the model tends to underestimate high-demand events a bit more often than it over predicts them.\n\n#### **Top 10 Feature Importances**\nThis bar chart shows the relative contribution of each feature to the Decision Tree's final prediction.\n* **Finding:** **`temp`and **`yr`  are the dominant predictors. This exhibits that people are more likely to bike when the weather is warmer, and the rise in yr (comparing 2011 to 2012) reflects how the bike-sharing system became noticeably more popular over time.","metadata":{}},{"cell_type":"markdown","source":"<div class=\"alert alert-block alert-info\">\n<h1 align=\"center\"><font color='green'>Summary of Regression Analysis on Bike Sharing Dataset</font></h1>\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"This exercise 1 addresses the prediction of the total daily bike rentals (cnt). I used a data set containing weather and temporal information which was perfectly suitable since the target (cnt) is a continuous numeric count. The entire dataset has 731 records and 16 features.\n\n#### Preprocessing and Feature Engineering\nThe initial processing was cleaning the data and lessening the data leakage, which signifies when the model accidentally cheats by seeing the answer before it concludes.\nThe 'instant', 'dteday' was removed along with 'casual' and 'registered' columns. This was crucial as they make up the total count (cnt = casual registered). Then, I used One-Hot Encoding to handle my categorical data, such as 'season', 'weekday', and 'weathersit'.\nFinally, the continuous variables like 'temp', 'hum', and'windspeed' were normalized using StandardScaler. This ensures that every feature contributes equally, especially for the Ridge model. Moreover, I split the data 80/20 into training and testing sets to properly check how well the model generalizes.\n\n\n#### Model Implementation and Evaluation\n\nI picked two models that were the best suited for this data, which follows:\n1.  **Decision Tree Regressor:** I chose this model because it fits well by capturing non-linear relationships and usually offers the highest predictive accuracy.\n2.  **Ridge Regression:** This was my reliable linear baseline model. It uses L2 regularization, which helps keep things stable when features might be correlated.\n\nThe models were evaluated using the standard regression metrics: **Mean Absolute Error (MAE)**, **Mean Squared Error (MSE)**, and the **R¬≤ score**.\n\n| Model | R¬≤ Train | R¬≤ Test | MAE Test | MSE Test |\n| :--- | :--- | :--- | :--- | :--- |\n| Decision Tree | 0.941 | **0.871** | 473.4 | 425,000 |\n| Ridge Regression | 0.816 | 0.826 | 639.1 | 599,000 |\n\n#### Results and Interpretation\n\nThe **Decision Tree Regressor** significantly outperformed the linear baseline, achieving an $R^2$ score of **0.871** on the test data. This means it approximately explains about **87.1%** of the variability in total daily bike rentals. The Ridge Regression model achieved an $R^2$ of **0.826**, showing an interpretable linear baseline.\n\nInsights:\n1.  **Generalization:** The gap between the Decision Tree's training $R^2$ (0.941) and testing $R^2$ (0.871) suggests tha model has a tendency to **overfit**. I already capped the tree depth at 10 to manage, however it shows how easily this non-linear model can memorize the training data.\n2.  **Feature Importance:** As shown in the visualization, 'temp' and 'year' are the strongest predictors. This reflects the dependence of biking activity when it is in warm weather and the overall growth of the program.\n3.  **Error Analysis:** The error plots **Predicted vs. Actual** shows a high accuracy, centering perfectly around zero, which confirms the model is unbiased. The only slight issue is that the residual distribution is slightly skewed, meaning the model tends to under-predict the data with the absolute highest rental demand.\n\n\nIn conclusion, the **Decision Tree Regressor** offers the highest predictive accuracy, while **Ridge Regression** provides a stable and reliable linear model. \n","metadata":{}}]}