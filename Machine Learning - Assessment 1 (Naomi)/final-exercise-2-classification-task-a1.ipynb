{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13433607,"sourceType":"datasetVersion","datasetId":8526498}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\"\"\"\n<h1 align=\"center\"><font color='green'> ðŸ§  Exercise 2: Brain Tumor Classification Task</font></h1>\n<h4 align=\"left\"> <font color='purple'> This project addresses a clinical classification task that predicts the outcome of an MRI scan (Positive or Negative) based on patient and tumor characteristics. The main aim is to evaluate Logistic Regression, Random Forest and SVM algorithms to determine the most reliable model.\nInitially, preprocessing steps include encoding categorical data, scaling, and addressing class imbalance through class weights. Models are evaluated using Accuracy, Precision, Recall, F1-score, and visualized with Confusion Matrices, ROC curves, and Precision-Recall curves. </font></h4>\n","metadata":{}},{"cell_type":"code","source":"#importing necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\n#Classification Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, roc_curve, auc, precision_recall_curve, ConfusionMatrixDisplay\nfrom scipy.interpolate import make_interp_spline\nimport warnings\n\n#suppressing a specific warning related to model fitting\nwarnings.filterwarnings('ignore', category=UserWarning)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T09:14:26.928789Z","iopub.execute_input":"2025-10-22T09:14:26.929124Z","iopub.status.idle":"2025-10-22T09:14:29.904841Z","shell.execute_reply.started":"2025-10-22T09:14:26.929103Z","shell.execute_reply":"2025-10-22T09:14:29.903956Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<div style=\"\n    background-color: #ffd6eb;\n    border-left: 6px solid #ff69b4;\n    padding: 20px;\n    border-radius: 10px;\n    color: #880e4f;\n    font-family: 'Helvetica Neue', sans-serif;\n    box-shadow: 0 2px 6px rgba(255, 105, 180, 0.3);\n\">\n    <h1 align=\"center\">\n        <font color='gray'>Data Loading and Preprocessing</font>\n    </h1>\n    <h4 align=\"center\">\n        <font color='blue'>\n            This section will focus on cleaning the dataset and transforming features so they're ready for modeling. These include: mapping the string-based target variable (MRI_Result) to numerical values (1 and 0), separating all variables (numeric and categorical), and splitting the data into 75% training and 25% testing sets using stratified sampling to ensure the proportion of positive vs negative is identical.\nLastly, 'ColumnTransformer' will be applied, which will handle standard scaling for numerical values and One-hot Encoding for nominal categorical features.\n        </font>\n    </h4>\n</div>","metadata":{}},{"cell_type":"code","source":"# --- Data Loading and Preprocessing ---\n\ndf = pd.read_csv(\"/kaggle/input/braintumor/brain_tumor_dataset.csv\")\n\n#dropping the non-predictive identifier column (cleaning)\ndf.drop('Patient_ID', axis=1, inplace=True)\n\n#My target variable is 'MRI_Result' (Positive/Negative)\ndf.rename(columns={'MRI_Result': 'Diagnosis_Result'}, inplace=True)\n\n#Map 'Positive' to 1 means Tumor Indicated and 'Negative' to 0 means No Tumor Indicated\ndf['Diagnosis_Result'] = df['Diagnosis_Result'].map({'Positive': 1, 'Negative': 0})\n\nprint(\"Loaded Dataset Shape:\", df.shape)\nprint(\"\\nClass Distribution (Target Variable 'Diagnosis_Result'):\")\nprint(df['Diagnosis_Result'].value_counts())\nprint(\"\\nFirst 5 rows after initial cleanup:\")\nprint(df.head())\n\ny = df['Diagnosis_Result']\nX = df.drop('Diagnosis_Result', axis=1)\n\n#These will be categorical for One-Hot Encoding\nnumeric_features = ['Age', 'Tumor_Size', 'Survival_Rate', 'Tumor_Growth_Rate']\n\ncategorical_features = [\n    'Gender', 'Tumor_Type', 'Location', 'Histology', 'Stage',\n    'Symptom_1', 'Symptom_2', 'Symptom_3', 'Radiation_Treatment',\n    'Surgery_Performed', 'Chemotherapy', 'Family_History', 'Follow_Up_Required'\n]\n\n# --- Stratified Train-Test Split ---\n#stratify=y to ensure both train/test sets maintain the original class proportion.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n\nprint(f\"\\nTrain set size: {X_train.shape[0]} records\")\nprint(f\"Test set size: {X_test.shape[0]} records\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T09:14:29.906778Z","iopub.execute_input":"2025-10-22T09:14:29.907305Z","iopub.status.idle":"2025-10-22T09:14:29.976654Z","shell.execute_reply.started":"2025-10-22T09:14:29.907282Z","shell.execute_reply":"2025-10-22T09:14:29.975573Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Preprocessing - Scaling and Encoding  ---\n\nnumeric_transformer = StandardScaler() #Standard Scaling for numerical consistency\ncategorical_transformer = OneHotEncoder(handle_unknown='ignore') #One-Hot Encoding for nominal data\n\n#column transformer that applies the correct transformation to each column set\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ],\n    remainder='passthrough'\n)\n\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n\nprint(\"\\nPreprocessing Complete: Categorical features encoded and numerical features scaled.\")\n\n#feature names after one-hot encoding\nfeature_names = numeric_features + list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features))\n\nX_train_df = pd.DataFrame(X_train_processed, columns=feature_names)\nX_test_df = pd.DataFrame(X_test_processed, columns=feature_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T09:14:29.977689Z","iopub.execute_input":"2025-10-22T09:14:29.977981Z","iopub.status.idle":"2025-10-22T09:14:30.01596Z","shell.execute_reply.started":"2025-10-22T09:14:29.977958Z","shell.execute_reply":"2025-10-22T09:14:30.014293Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n<div style=\"\n    background-color: #ffd6eb;\n    border-left: 6px solid #ff69b4;\n    padding: 20px;\n    border-radius: 10px;\n    color: #880e4f;\n    font-family: 'Helvetica Neue', sans-serif;\n    box-shadow: 0 2px 6px rgba(255, 105, 180, 0.3);\n\">\n    <h1 align=\"center\">\n        <font color='gray'>Model Implementation and Evaluation</font>\n    </h1>\n    <h4 align=\"center\">\n        <font color='blue'>\n            With the data fully processed, we will now train the models using 3 classification algorithms (Logistic Regression, Random Forest, and SVM)\nTo address the critical clinical nature of this problem and potential class imbalance, I will set **`class_weight='balanced'`** in all models to ensure the models prioritize the detection of true tumor cases.\n        </font>\n    </h4>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# --- Model Implementation and Evaluation ---\n\n#Interpretable linear baseline (Logistic Regression)\nlog_reg = LogisticRegression(random_state=42, solver='liblinear', class_weight='balanced') \nlog_reg.fit(X_train_df, y_train)\n\n#Robust non-linear ensemble model (Random Forest Classifier)\nrand_forest = RandomForestClassifier(random_state=42, class_weight='balanced', max_depth=10) \nrand_forest.fit(X_train_df, y_train)\n\n#Support Vector Machine (SVM), needs probability estimates for ROC/PR curves, so setting probability=True\nsvm_model = SVC(random_state=42, probability=True, kernel='rbf', class_weight='balanced')\nsvm_model.fit(X_train_df, y_train)\n\nmodels = {\n    \"Logistic Regression\": log_reg,\n    \"Random Forest\": rand_forest,\n    \"SVM\": svm_model\n}\n\nresults = []\n\n#Loop to calculate metrics\nfor name, model in models.items():\n    y_pred = model.predict(X_test_df)\n\n    #Calculate key classification metrics\n    accuracy = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, zero_division=0)\n    recall = recall_score(y_test, y_pred, zero_division=0)\n    f1 = f1_score(y_test, y_pred, zero_division=0)\n\n    results.append({\n        'Model': name,\n        'Accuracy': f\"{accuracy:.3f}\",\n        'Precision': f\"{precision:.3f}\",\n        'Recall': f\"{recall:.3f}\",\n        'F1-Score': f\"{f1:.3f}\",\n    })\n\nresults_df = pd.DataFrame(results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T09:14:30.01729Z","iopub.execute_input":"2025-10-22T09:14:30.01865Z","iopub.status.idle":"2025-10-22T09:14:30.419807Z","shell.execute_reply.started":"2025-10-22T09:14:30.018613Z","shell.execute_reply":"2025-10-22T09:14:30.418916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background-color: #ffd6eb;\n    border-left: 6px solid #ff69b4;\n    padding: 20px;\n    border-radius: 10px;\n    color: #880e4f;\n    font-family: 'Helvetica Neue', sans-serif;\n    box-shadow: 0 2px 6px rgba(255, 105, 180, 0.3);\n\">\n    <h1 align=\"center\">\n        <font color='gray'>Visualization and Interpretation</font>\n    </h1>\n    <h4 align=\"center\">\n        <font color='blue'>\n            The final step is to visualize and interpret the results to draw a definitive conclusion. I have visualized the Confusion Matrix for the best-performing model (Random Forest) to clearly see the number of False Positives and False Negatives, which is vital for clinical interpretation.\nThen, generate the ROC and the Precision-Recall curve to assess their performance specifically on the positive class.\n        </font>\n    </h4>\n</div>\n","metadata":{}},{"cell_type":"code","source":"# --- Visualization and Interpretation ---\n\nprint(\"\\n--- Model Performance Comparison (Test Set) ---\")\nprint(results_df.to_markdown(index=False))\n\n# --- Visualizations 1,2,3: Confusion Matrix for the Best Model (Random Forest), ROC Curve and Precision-Recall Curve ---\nbest_model_name = \"Random Forest\"\nbest_model = rand_forest\ny_pred_best = best_model.predict(X_test_df)\n\nplt.figure(figsize=(6, 5))\nConfusionMatrixDisplay.from_estimator(best_model, X_test_df, y_test, cmap=plt.cm.Blues, display_labels=['Negative (0)', 'Positive (1)'])\nplt.title(f'Confusion Matrix: {best_model_name} (Best F1-Score)')\nplt.grid(False)\nplt.show()\n\nplt.figure(figsize=(15, 6))\n\n#ROC Curve \nplt.subplot(1, 2, 1)\nplt.plot([0, 1], [0, 1], 'k--', label='Baseline (AUC = 0.50)')\n\nfor name, model in models.items():\n    #obtaining probability estimates\n    if name == \"SVM\":\n        y_prob = model.predict_proba(X_test_df)[:, 1]\n    else:\n        y_prob = model.predict_proba(X_test_df)[:, 1]\n\n    fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.3f})')\n\nplt.title('Receiver Operating Characteristic (ROC) Curve', fontsize=14)\nplt.xlabel('False Positive Rate (1 - Specificity)', fontsize=12)\nplt.ylabel('True Positive Rate (Recall)', fontsize=12)\nplt.legend(loc=\"lower right\")\nplt.grid(True, linestyle='--')\n\n\n#Precision-Recall Curve\nplt.subplot(1, 2, 2)\n#Calculate baseline\nno_skill = len(y_test[y_test==1]) / len(y_test)\nplt.plot([0, 1], [no_skill, no_skill], linestyle='--', label=f'Baseline (Precision = {no_skill:.3f})')\n\nfor name, model in models.items():\n    if name == \"SVM\":\n        y_prob = model.predict_proba(X_test_df)[:, 1]\n    else:\n        y_prob = model.predict_proba(X_test_df)[:, 1]\n\n    #Calculate Precision-Recall curve\n    precision, recall, _ = precision_recall_curve(y_test, y_prob)\n    plt.plot(recall, precision, label=name)\n\nplt.title('Precision-Recall Curve', fontsize=14)\nplt.xlabel('Recall (True Positive Rate)', fontsize=12)\nplt.ylabel('Precision (Positive Predictive Value)', fontsize=12)\nplt.legend(loc='lower left')\nplt.grid(True, linestyle='--')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-22T09:14:30.421709Z","iopub.execute_input":"2025-10-22T09:14:30.421953Z","iopub.status.idle":"2025-10-22T09:14:31.293348Z","shell.execute_reply.started":"2025-10-22T09:14:30.421934Z","shell.execute_reply":"2025-10-22T09:14:31.292093Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"\n    background-color: #ffd6eb;\n    border-left: 6px solid #ff69b4;\n    padding: 20px;\n    border-radius: 10px;\n    color: #880e4f;\n    font-family: 'Helvetica Neue', sans-serif;\n    box-shadow: 0 2px 6px rgba(255, 105, 180, 0.3);\n\">\n    <h1 align=\"center\">\n        <font color='gray'>Final Summary and Interpretation (Brain Tumor Diagnosis)</font>\n    </h1>\n    <h4 align=\"left\">\n        <font color='#006699'>\nThis classification task was to build a reliable model for predicting a positive MRI result, indicating a likely brain tumor. I used a clinical dataset of approximately 795 patient records with features like age, tumor characteristics, histology, and symptoms.\n\n<h3 align=\"center\">\n        <font color='gray'>Preprocessing and Imbalance Handling</font>\n    </h3>\n\nI immediately focused on data preparation. I defined my target variable as Diagnosis_Result from the original (MRI_Result) and mapped 'Positive' cases to 1 and 'Negative' to 0.Â \nI applied stratified splitting and balanced class weights since dealing with rare conditions often means dealing with imbalance.\n1.Â  Stratified Splitting: This ensured both my training and testing sets accurately produce the original distribution of positive and negative cases.\n2.Â  Balanced Class Weights: I instructed all algorithms (`class_weight='balanced') to heavily penalize missing a patient who actually had a positive MRI (False Negative).\n\nMoreover, I used Standard Scaling for numerical data and One-Hot Encoding for the numerous categorical features, making sure all features contributed equally.\n\n<h3 align=\"center\">\n        <font color='gray'>Model Implementation and Evaluation </font>\n    </h3>\n\nFor model implementation and evaluation, I used Logistic Regression, Random Forest, and SVM. The clinical priority is absolute: Recall is paramount. I must find as many true positive cases as possible to avoid a missed diagnosis. My evaluation, therefore, prioritizes the F1-Score, as it balances this high recall goal with necessary precision.\n\n| Model | Accuracy | Precision | Recall | F1-Score |\n| :--- | :--- | :--- | :--- | :--- |\n| Random Forest | 0.565 | 0.530 | 0.490 | 0.508 |\n| Logistic Regression | 0.540 | 0.511 | 0.470 |0.490 |\n| SVM | 0.555 | 0.525 | 0.485 | 0.504 |\n\n <h3 align=\"center\">\n        <font color='gray'> Interpretation and Limitations</font>\n    </h3>\n\nThe Random Forest Classifier achieved the highest overall F1-score (0.508). This score shows that the models are only marginally better than random chance (0.50 F1-score). This shows a low predictive power based on the current features.\n\nROC curve showed that all models clustered very close to the no-skill line (AUC $\\approx 0.55$), indicating they struggled with overall discriminatory power. Furthermore, the Precision-Recall curve was close to the baseline as they could not maintain high precision when pushed to achieve maximum recall. (weak performance)Â \n\n <h3 align=\"center\">\n        <font color='gray'> Limitations</font>\n    </h3>\nFor model implementation and evaluation, I used Logistic Regression, Random Forest, and SVM. The clinical priority is absolute: Recall is paramount. I must find as many true \n1.Â  Underfitting: Given the low scores across all metrics, the models are most likely underfitting the data. This means the existing features or scaling and encoding are not providing enough signal for the models. The models are simply too close to random guessing.\n2.Â  Feature Complexity: I handled all the categorical features with One-Hot Encoding, but all those resulting columns might actually be drowning out my strongest numerical predictors. (underfit)\n\nOverall, the current iteration of the models does not yet prove to have high reliability. The Random Forest slightly stands out, but further feature engineering and hyperparameter tuning are critically needed to raise the F1-score.\n    </h4>\n</div>\n\n\n\n","metadata":{}}]}